{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "EN_train = 'EN/train'\n",
    "SG_train = 'SG/train'\n",
    "CN_train = 'CN/train'\n",
    "EN_test = 'EN/dev.in'\n",
    "SG_test = 'SG/dev.in'\n",
    "CN_test = 'CN/dev.in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "181628\n27225\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    word\n",
       "0    HBO\n",
       "1    has\n",
       "2  close\n",
       "3     to\n",
       "4     24"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HBO</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>has</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>close</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>to</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "def load_train(training_file):\n",
    "    df = pd.read_csv(training_file, sep=' ', header = None, error_bad_lines=False)\n",
    "    df.columns=['word','state']\n",
    "    return df\n",
    "\n",
    "def load_test(test_file):\n",
    "    ls = []\n",
    "    f = open(test_file,encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        ls.append(line.strip('\\n'))\n",
    "    df_test = pd.DataFrame(ls)\n",
    "    df_test.columns=['word']\n",
    "    return df_test\n",
    "        \n",
    "# df_test = load_test(EN_test)\n",
    "# print(len(df_test))\n",
    "df_train = load_train(EN_train)\n",
    "print(len(df_train))\n",
    "df_train.head(5)\n",
    "\n",
    "df_test = load_test(EN_test)\n",
    "print(len(df_test))\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time elapsed 0.8995108604431152 seconds\n"
     ]
    }
   ],
   "source": [
    "def createMatrix(df):\n",
    "    start = time.time()\n",
    "    columns = df.word.unique().tolist()\n",
    "    index = df.state.unique().tolist()\n",
    "    new_df = pd.DataFrame(columns=columns, index=index)\n",
    "    print(f'time elapsed {time.time()-start} seconds')\n",
    "    return new_df\n",
    "empty_matrix = createMatrix(df_train)    \n",
    "# emission_matrix.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "21\n18212\n"
     ]
    }
   ],
   "source": [
    "print(len(empty_matrix))\n",
    "print(len(empty_matrix.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "25051it [04:08, 100.79it/s]\n",
      "time elapsed 250.04122018814087\n"
     ]
    }
   ],
   "source": [
    "def emissionMatrix_special(df, emission_matrix):\n",
    "    k=0.5\n",
    "    start = time.time()\n",
    "    df_denominator = df.groupby('state').count()   # getting counts of states\n",
    "    df_counts = df.groupby(['state','word']).size().reset_index()   # getting counts of every word in each state\n",
    "    df_merged = df_counts.merge(df_denominator, left_on=['state'], right_on='state')  # merge \n",
    "    df_merged = df_merged.rename(columns={\"word_x\": \"word\",0:\"word_count\", \"word_y\": \"state_count\"})\n",
    "    df_merged['Probability'] = df_merged.word_count/(df_merged.state_count+k)    # get emission probability (count of word in that state/ state count)\n",
    "    for index, row in tqdm(df_merged.iterrows()):  # for every known probabilty\n",
    "        emission_matrix.loc[row['state'],row['word']] = row['Probability']   # append into the emission matrix\n",
    "    for i in df_train.state.unique().tolist():\n",
    "        emission_matrix.loc[i,'#UNK#'] = float(k/df_denominator.loc[i]+k)\n",
    "    emission_matrix = emission_matrix.fillna(0)   # fill those null cells with zero\n",
    "    print(f'time elapsed {time.time()-start}')\n",
    "    return emission_matrix\n",
    "\n",
    "emission_matrix = emissionMatrix_special(df_train, empty_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ab805ef2381e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memission_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtag_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ab805ef2381e>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3957\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3958\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "def argmax(df):\n",
    "    start = time.time()\n",
    "    tags={}\n",
    "    for col in df.columns:\n",
    "        tags[col]=[df[col].argmax()]\n",
    "    return tags\n",
    "        \n",
    "tags = argmax(emission_matrix)\n",
    "\n",
    "def tag_system(tag_dict, test_df):\n",
    "    start = time.time()\n",
    "    test_ls = test_df['word'].tolist()\n",
    "    tag_states=[]\n",
    "    for i in test_ls:\n",
    "        if i in tag_dict.keys():\n",
    "            tag_states.append(tag_dict[i])\n",
    "        elif i==\"\":   # for blank lines, set state to be blank\n",
    "            tag_states.append(\"\")\n",
    "        elif i not in tag_dict.keys():\n",
    "            tag_states.append(tag_dict['#UNK#'])\n",
    "\n",
    "    test_df['states']=tag_states\n",
    "    print(f'time elapsed {time.time()-start}')\n",
    "    return test_df\n",
    "output = tag_system(tags,df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_trans(training_file):\n",
    "    f = open(training_file)\n",
    "    ls_state = ['START']\n",
    "    for line in f:\n",
    "        item = line.strip('\\n').split(' ')\n",
    "        if len(item) == 2:\n",
    "            ls_state.append(item[1])\n",
    "        elif len(item) < 2:\n",
    "            ls_state.append('STOP')\n",
    "            ls_state.append('START')\n",
    "    ls_state.pop(-1)\n",
    "    return ls_state\n",
    "\n",
    "def relation_matrix(temp):\n",
    "    count = Counter(temp)\n",
    "    list_key = list(count.keys())\n",
    "    rls_matrix = pd.DataFrame(columns=list_key, index=list_key)\n",
    "    for (x, y), c in Counter(zip(temp, temp[1:])).items():\n",
    "        rls_matrix.loc[[x], [y]] = c/count[x]\n",
    "    rls_matrix = rls_matrix.fillna(value=0)\n",
    "    rls_matrix = rls_matrix.drop(columns='START')\n",
    "    rls_matrix = rls_matrix.drop(index='STOP')\n",
    "    return rls_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_ls = load_train_trans(EN_train)\n",
    "transition_matrix = relation_matrix(sequence_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transition_matrix.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emission_matrix; transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = open('EN/dev.in', encoding=\"utf8\")\n",
    "ls=[]\n",
    "big_ls=[]\n",
    "for line in m:\n",
    "    item=line.strip('\\n')\n",
    "    if item=='':\n",
    "        big_ls.append(ls)\n",
    "        ls=[]\n",
    "    elif item!='':\n",
    "        ls.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiple=[] # will conrain\n",
    "for i in big_ls:  #for each sentene\n",
    "    Vertibri=[] # will contain weights for all layers within each document\n",
    "    forward_steps = len(i)+1 \n",
    "    for j in forward_steps: #for each layer\n",
    "        if j==0:  #start to first layer\n",
    "            layer_v= [a*b for a,b in zip(transition_matrix.,[all states --> i[j] #word in sentence])]\n",
    "            big_v.append(small_v)\n",
    "        elif j!=0 & j!=forward_steps: #not first or last step\n",
    "            small_v=[a*b*c for a,b,c in zip([small_v],[state-> state],[ state -> word])]\n",
    "            big_v.append(small_v)\n",
    "        else: #if last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log(x, inf_replace=-1000):\n",
    "    out = np.log(x)\n",
    "    out[~np.isfinite(out)] = inf_replace\n",
    "    return out\n",
    "logged_emission = log(emission_matrix)\n",
    "logged_transition = log(transition_matrix)\n",
    "transition_np = logged_transition.drop(['START']).drop('STOP',axis=1).to_numpy()\n",
    "\n",
    "# test for one document\n",
    "tags = argmax(emission_matrix)   # vocab of words\n",
    "Vertibri = []\n",
    "document = big_ls[1]\n",
    "# print(document)\n",
    "forward_steps = len(document)+1\n",
    "for i in range(forward_steps):\n",
    "    if i == 0: # for from START to first layer\n",
    "        if document[i] in tags.keys():\n",
    "            layer = [t+e for t,e in zip(logged_transition.loc['START'].drop('STOP'), logged_emission[document[i]])]\n",
    "        elif document[i] not in tags.keys():\n",
    "            layer = [t+e for t,e in zip(logged_transition.loc['START'].drop('STOP'), logged_emission['#UNK#'])]\n",
    "        Vertibri.append(layer)\n",
    "        print(type(Vertibri[-1]))\n",
    "    elif i!=0 and i!=forward_steps-1: #not first or last step\n",
    "        prev_layer_prob = Vertibri[-1]*21\n",
    "        prev_layer_prob = np.array(prev_layer_prob).reshape(21,21).T\n",
    "        m = prev_layer_prob + transition_np\n",
    "        if document[i] in tags.keys():\n",
    "            emission_ls = logged_emission[document[i]].tolist()*21\n",
    "            emission_np = np.array(emission_ls).reshape(21,21)\n",
    "        elif document[i] not in tags.keys():\n",
    "            emission_ls = logged_emission['#UNK#'].tolist()*21\n",
    "            emission_np = np.array(emission_ls).reshape(21,21)\n",
    "        matrix = (m + emission_np)\n",
    "        layer = np.amax(matrix,0)\n",
    "        Vertibri.append(layer.tolist())\n",
    "    elif i == forward_steps-1:\n",
    "        prev_layer_prob = np.array(Vertibri[-1])\n",
    "        last = logged_transition.drop('START')['STOP'].tolist()\n",
    "        layer = prev_layer_prob+last\n",
    "        Vertibri.append(layer.tolist())\n",
    "\n",
    "state_order = []\n",
    "states = emission_matrix.index.tolist()\n",
    "# Vertibri.pop(0)\n",
    "for layer in Vertibri:\n",
    "    position = layer.index(max(layer))\n",
    "    state_order.append(states[position])\n",
    "# state_order = []\n",
    "# states = emission_matrix.index.tolist()\n",
    "# for layer in Vertibri:\n",
    "#     position = layer.index[max(layer)]\n",
    "#     # states = emission_matrix.index.tolist()\n",
    "#     state_order.append(states[position])\n",
    "# print(state_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(Vertibri.pop(0))\n",
    "for layer in Vertibri:\n",
    "    print(layer.index(max(layer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_order = []\n",
    "states = emission_matrix.index.tolist()\n",
    "Vertibri.pop(0)\n",
    "for layer in Vertibri:\n",
    "    position = layer.index(max(layer))\n",
    "    # states = emission_matrix.index.tolist()\n",
    "    state_order.append(states[position])\n",
    "print(state_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Vertibri))\n",
    "print(Vertibri[11])\n",
    "print(Vertibri[11].index(max(Vertibri[11])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_layer_prob = np.array(Vertibri[-2])\n",
    "print(prev_layer_prob)\n",
    "print(logged_transition.drop('START')['STOP'].tolist())\n",
    "\n",
    "print(prev_layer_prob + logged_transition.drop('START')['STOP'].tolist() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition_np = logged_transition.drop(['START']).drop('STOP',axis=1).to_numpy()\n",
    "# ls = Vertibri[-1]*21\n",
    "# prev_np = np.array(ls).reshape(21,21).T\n",
    "# print(prev_np)\n",
    "# print(Vertibri[-1].tolist()*21)\n",
    "# ls = logged_emission[document[3]].tolist()*21\n",
    "# emission_np = np.array(ls).reshape(21,21)\n",
    "# print(emission_np)\n",
    "\n",
    "print(logged_transition.drop('START').index.tolist())\n",
    "print(emission_matrix.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags = argmax(emission_matrix)   # vocab of words\n",
    "# Vertibri = []\n",
    "# document = big_ls[1]\n",
    "# # print(document)\n",
    "# forward_steps = len(document)+1\n",
    "# for i in range(forward_steps):\n",
    "#     if i == 0: # for from START to first layer\n",
    "#         if document[i] in tags.keys():\n",
    "#             layer = [t*e for t,e in zip(transition_matrix.loc['START'].drop('STOP'), emission_matrix[document[i]])]\n",
    "#         elif document[i] not in tags.keys():\n",
    "#             layer = [t*e for t,e in zip(transition_matrix.loc['START'].drop('STOP'), emission_matrix['#UNK#'])]\n",
    "#         Vertibri.append(layer)\n",
    "#         break\n",
    "# print(Vertibri[0])\n",
    "# print(Vertibri[0].index(max(Vertibri[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = big_ls[1]\n",
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ls = [1/6]*3 + [0]*3 + [1/16]*3\n",
    "# print(ls)\n",
    "prev = np.array(ls).reshape(3,3)\n",
    "\n",
    "ls1 = [1/6,0,4/6,1/4,0,0,1/8,4/8,1/8]\n",
    "trans = np.array(ls1).reshape(3,3)\n",
    "\n",
    "ls2 = [1/6,1/4,1/8]*3\n",
    "print(len(ls))\n",
    "em = np.array(ls2).reshape(3,3)\n",
    "print('prev prob matrix')\n",
    "print(prev)\n",
    "print('trans matrix')\n",
    "print(trans)\n",
    "print('em matrix')\n",
    "print(em)\n",
    "\n",
    "output = np.multiply(prev,trans)\n",
    "output = np.multiply(output,em)\n",
    "print('next prob matrix')\n",
    "print(output)\n",
    "\n",
    "print(np.amax(output,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "\t[[21, 72, 67],\n",
    "\t[23, 78, 69],\n",
    "\t[32, 74, 56],\n",
    "\t[52, 54, 76]],\n",
    "\tcolumns=['a', 'b', 'c'])\n",
    "\n",
    "print('DataFrame\\n----------\\n', df)\n",
    "\n",
    "#convert dataframe to numpy array\n",
    "arr = df.to_numpy()\n",
    "\n",
    "print('\\nNumpy Array\\n----------\\n', arr)\n",
    "\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}