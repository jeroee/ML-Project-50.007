{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/lichang/anaconda3/lib/python3.7/site-packages (3.8.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/lichang/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/lichang/anaconda3/lib/python3.7/site-packages (from gensim) (4.0.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/lichang/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/lichang/anaconda3/lib/python3.7/site-packages (from gensim) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "!pip install gensim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(train_file):\n",
    "    # Split train file into word and state lists\n",
    "    word_ls = []\n",
    "    st_ls = []\n",
    "    f = open(train_file, encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        line_2 = str(line.strip('\\n'))\n",
    "        if line_2 != '':\n",
    "            inter_ls = line_2.split()\n",
    "            word_ls.append(inter_ls[0])\n",
    "            st_ls.append(inter_ls[1])\n",
    "        elif line_2 == '':\n",
    "            word_ls.append(line_2)\n",
    "            st_ls.append('')\n",
    "    \n",
    "    compiled_word_ls = []\n",
    "    compiled_state_ls = []\n",
    "    big_word_ls = []\n",
    "    big_state_ls = []\n",
    "    for i in range(len(word_ls)):\n",
    "        if word_ls[i] != '':\n",
    "            compiled_word_ls.append(word_ls[i])\n",
    "            compiled_state_ls.append(st_ls[i])\n",
    "        elif word_ls[i] == '':\n",
    "            big_word_ls.append(compiled_word_ls)\n",
    "            big_state_ls.append(compiled_state_ls)\n",
    "            compiled_word_ls = []\n",
    "            compiled_state_ls = []\n",
    "    return big_word_ls, big_state_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Files and split them into respective lists\n",
    "train_x_ls, train_y_ls = load_file('train')\n",
    "test_x_ls, test_y_ls = load_file('dev.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tagged sentences: 7663\n",
      "Vocabulary size: 16490\n",
      "Total number of tags: 21\n"
     ]
    }
   ],
   "source": [
    "## Count your unique words and tags\n",
    "num_words = len(set([word.lower() for sentence in train_x_ls for word in sentence]))\n",
    "num_tags   = len(set([word.lower() for sentence in train_y_ls for word in sentence]))\n",
    "print(\"Total number of tagged sentences: {}\".format(len(train_x_ls)))\n",
    "print(\"Vocabulary size: {}\".format(num_words))\n",
    "print(\"Total number of tags: {}\".format(num_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode X\n",
    "\n",
    "word_tokenizer = Tokenizer()                      # instantiate tokeniser\n",
    "word_tokenizer.fit_on_texts(train_x_ls)                    # fit tokeniser on data\n",
    "X_train_encoded = word_tokenizer.texts_to_sequences(train_x_ls)  # use the tokeniser to encode input sequence\n",
    "X_test_encoded = word_tokenizer.texts_to_sequences(test_x_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode Y\n",
    "\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(train_y_ls)\n",
    "Y_train_encoded = tag_tokenizer.texts_to_sequences(train_y_ls)\n",
    "Y_test_encoded = tag_tokenizer.texts_to_sequences(test_y_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dic = tag_tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve your state dictionary for mapping\n",
    "state_dic = tokenizer_dic['index_word']\n",
    "state_dic = ast.literal_eval(state_dic)\n",
    "state_dic = {k:v.upper() for k,v in state_dic.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'I-NP',\n",
       " '2': 'B-NP',\n",
       " '3': 'O',\n",
       " '4': 'B-PP',\n",
       " '5': 'B-VP',\n",
       " '6': 'I-VP',\n",
       " '7': 'B-ADVP',\n",
       " '8': 'B-SBAR',\n",
       " '9': 'B-ADJP',\n",
       " '10': 'I-ADJP',\n",
       " '11': 'B-PRT',\n",
       " '12': 'I-ADVP',\n",
       " '13': 'I-PP',\n",
       " '14': 'I-CONJP',\n",
       " '15': 'B-CONJP',\n",
       " '16': 'I-SBAR',\n",
       " '17': 'B-INTJ',\n",
       " '18': 'B-LST',\n",
       " '19': 'I-INTJ',\n",
       " '20': 'I-UCP',\n",
       " '21': 'B-UCP'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each sequence to MAX_SEQ_LENGTH using KERAS' pad_sequences() function. \n",
    "# Sentences longer than MAX_SEQ_LENGTH are truncated.\n",
    "# Sentences shorter than MAX_SEQ_LENGTH are padded with zeroes.\n",
    "\n",
    "# Truncation and padding can either be 'pre' or 'post'. \n",
    "# For padding we are using 'pre' padding type, that is, add zeroes on the left side.\n",
    "# For truncation, we are using 'post', that is, truncate a sentence from right side.\n",
    "\n",
    "MAX_SEQ_LENGTH = 100  # sequences greater than 100 in length will be truncated\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "X_test_padded = pad_sequences(X_test_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "Y_train_padded = pad_sequences(Y_train_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "Y_test_padded = pad_sequences(Y_test_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "# load word2vec using the following function present in the gensim library\n",
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign word vectors from word2vec model\n",
    "\n",
    "EMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "# create an empty embedding matix\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "# create a word to index dictionary mapping\n",
    "word2id = word_tokenizer.word_index\n",
    "\n",
    "# copy vectors from word2vec model to the words present in corpus\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign padded sequences to X and Y\n",
    "X, Y = X_train_padded, Y_train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7663, 100, 22)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use Keras' to_categorical function to one-hot encode Y\n",
    "Y_oh = to_categorical(Y)\n",
    "Y_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1094, 100, 22)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use Keras' to_categorical function to one-hot encode Y\n",
    "Y_test_oh = to_categorical(Y_test_padded, 22)\n",
    "Y_test_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split entire data into training and testing sets\n",
    "TEST_SIZE = 0.2\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y_oh, test_size=TEST_SIZE, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          4947300   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 100, 22)           2838      \n",
      "=================================================================\n",
      "Total params: 5,137,018\n",
      "Trainable params: 5,137,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# total number of tags\n",
    "NUM_CLASSES = Y_oh.shape[2]\n",
    "\n",
    "# create architecture\n",
    "\n",
    "\n",
    "bidirect_model = Sequential()\n",
    "bidirect_model.add(Embedding(input_dim     = VOCABULARY_SIZE,\n",
    "                             output_dim    = EMBEDDING_SIZE,\n",
    "                             input_length  = MAX_SEQ_LENGTH,\n",
    "                             weights       = [embedding_weights],\n",
    "                             trainable     = True\n",
    "))\n",
    "bidirect_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "bidirect_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))\n",
    "bidirect_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "# check summary of model\n",
    "bidirect_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "48/48 [==============================] - 27s 561ms/step - loss: 1.1505 - acc: 0.7990 - val_loss: 0.4442 - val_acc: 0.8514\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 32s 675ms/step - loss: 0.3995 - acc: 0.8688 - val_loss: 0.3498 - val_acc: 0.8912\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 28s 584ms/step - loss: 0.2765 - acc: 0.9236 - val_loss: 0.2100 - val_acc: 0.9432\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 28s 583ms/step - loss: 0.1615 - acc: 0.9561 - val_loss: 0.1416 - val_acc: 0.9606\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 24s 507ms/step - loss: 0.1111 - acc: 0.9721 - val_loss: 0.1096 - val_acc: 0.9712\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 34s 708ms/step - loss: 0.0851 - acc: 0.9790 - val_loss: 0.0931 - val_acc: 0.9747\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 36s 743ms/step - loss: 0.0701 - acc: 0.9821 - val_loss: 0.0835 - val_acc: 0.9767\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 25s 515ms/step - loss: 0.0601 - acc: 0.9845 - val_loss: 0.0772 - val_acc: 0.9785\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 26s 545ms/step - loss: 0.0527 - acc: 0.9866 - val_loss: 0.0729 - val_acc: 0.9795\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 26s 551ms/step - loss: 0.0468 - acc: 0.9883 - val_loss: 0.0698 - val_acc: 0.9803\n"
     ]
    }
   ],
   "source": [
    "bidirect_training = bidirect_model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9, 86, 66, ..., 71, 71, 71],\n",
       "       [ 9, 94, 93, ..., 91, 91, 98],\n",
       "       [ 9, 96, 88, ..., 91, 98, 98],\n",
       "       ...,\n",
       "       [ 9, 91, 86, ..., 83, 76, 83],\n",
       "       [ 9, 96, 86, ..., 84, 84, 84],\n",
       "       [ 9, 95, 94, ..., 97,  0, 93]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bidirect_model.predict(X_test_padded)\n",
    "y_pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69, 91, 88, ..., 83, 81, 96])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_padded.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 58, 55, ...,  0,  0,  0],\n",
       "       [ 0, 94, 89, ...,  0,  0,  0],\n",
       "       [ 0, 92, 86, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0, 85, 75, ...,  0,  0,  0],\n",
       "       [ 0, 76, 75, ...,  0,  0,  0],\n",
       "       [ 0, 94, 93, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_oh.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ..., 15,  0,  1],\n",
       "       [ 0,  0,  0, ...,  6, 38,  1],\n",
       "       [ 0,  0,  0, ...,  2,  1,  8]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(Y_test_padded.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          50       0.00      0.00      0.00         0\n",
      "          53       0.00      0.00      0.00         1\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         1\n",
      "          56       0.00      0.00      0.00         2\n",
      "          57       0.00      0.00      0.00         1\n",
      "          58       0.00      0.00      0.00         3\n",
      "          59       0.00      0.00      0.00         2\n",
      "          60       0.00      0.00      0.00         5\n",
      "          61       0.50      1.00      0.67         1\n",
      "          62       0.00      0.00      0.00         3\n",
      "          63       0.00      0.00      0.00         3\n",
      "          64       0.00      0.00      0.00         4\n",
      "          65       0.43      0.50      0.46         6\n",
      "          66       0.12      0.20      0.15         5\n",
      "          67       0.00      0.00      0.00         9\n",
      "          68       0.18      0.25      0.21         8\n",
      "          69       0.09      0.14      0.11         7\n",
      "          70       0.25      0.25      0.25        12\n",
      "          71       0.31      0.33      0.32        12\n",
      "          72       0.00      0.00      0.00        17\n",
      "          73       0.26      0.38      0.31        13\n",
      "          74       0.41      0.32      0.36        22\n",
      "          75       0.39      0.39      0.39        23\n",
      "          76       0.35      0.33      0.34        21\n",
      "          77       0.41      0.32      0.36        28\n",
      "          78       0.25      0.24      0.24        25\n",
      "          79       0.28      0.29      0.29        24\n",
      "          80       0.34      0.45      0.39        22\n",
      "          81       0.34      0.31      0.33        32\n",
      "          82       0.38      0.41      0.39        37\n",
      "          83       0.25      0.17      0.20        30\n",
      "          84       0.29      0.45      0.35        31\n",
      "          85       0.41      0.32      0.36        37\n",
      "          86       0.54      0.55      0.54        51\n",
      "          87       0.38      0.32      0.34        47\n",
      "          88       0.37      0.43      0.40        47\n",
      "          89       0.47      0.58      0.52        45\n",
      "          90       0.48      0.44      0.46        57\n",
      "          91       0.53      0.62      0.57        47\n",
      "          92       0.42      0.40      0.41        52\n",
      "          93       0.51      0.42      0.46        53\n",
      "          94       0.58      0.52      0.55        58\n",
      "          95       0.49      0.49      0.49        53\n",
      "          96       0.43      0.48      0.45        42\n",
      "          97       0.38      0.71      0.50        21\n",
      "          98       0.76      0.61      0.68        62\n",
      "          99       0.67      0.73      0.70        11\n",
      "\n",
      "    accuracy                           0.41      1094\n",
      "   macro avg       0.28      0.30      0.28      1094\n",
      "weighted avg       0.42      0.41      0.41      1094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test_padded.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 2s 52ms/step - loss: 0.5808 - acc: 0.9036\n",
      "Loss: 0.5808255076408386,\n",
      "Accuracy: 0.9035557508468628\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = bidirect_model.evaluate(X_test_padded, Y_test_oh, verbose = 1)\n",
    "print(\"Loss: {0},\\nAccuracy: {1}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
